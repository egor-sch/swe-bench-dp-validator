name: Validate SWE-bench Data Point (Manual)

on:
  workflow_dispatch:
    inputs:
      data_point_names:
        description: 'Data point file name(s) to validate (comma-separated, without .json extension). Example: astropy__astropy-11693,astropy__astropy-11692'
        required: false
        default: 'astropy__astropy-11693'
        type: string

jobs:
  validate:
    runs-on: ubuntu-latest
    # Note: SWE-bench evaluation requires significant resources:
    # - Docker with sufficient disk space (~120GB+ recommended)
    # - At least 16GB RAM recommended
    # - 8+ CPU cores recommended
    # GitHub Actions free tier may have resource limitations.
    # Consider using self-hosted runners or GitHub-hosted larger runners for production use.
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"
      
      - name: Run validation
        id: validate
        continue-on-error: true
        run: |
          chmod +x scripts/validate_swe_bench.sh
          
          # Parse comma-separated data point names and build command
          VALIDATION_ARGS=""
          IFS=',' read -ra NAMES <<< "${{ inputs.data_point_names }}"
          for name in "${NAMES[@]}"; do
            # Trim whitespace and add .json extension if not present
            name=$(echo "$name" | xargs)
            if [[ ! "$name" == *.json ]]; then
              name="${name}.json"
            fi
            VALIDATION_ARGS="$VALIDATION_ARGS --data_point_names '$name'"
          done
          
          ./scripts/validate_swe_bench.sh $VALIDATION_ARGS --timeout 1800 --verbose
      
      - name: Find harness output directory
        id: find_outputs
        if: always()
        run: |
          # Find the most recent run directory
          RUN_DIR=$(find logs/run_evaluation -type d -name "validator_*" 2>/dev/null | sort -r | head -n 1)
          
          if [ -z "$RUN_DIR" ]; then
            echo "No run directory found"
            exit 0
          fi
          
          echo "run_dir=$RUN_DIR" >> $GITHUB_OUTPUT
          echo "Found run directory: $RUN_DIR"
      
      - name: Upload harness logs and outputs
        if: always() && steps.find_outputs.outputs.run_dir != ''
        uses: actions/upload-artifact@v4
        with:
          name: validation-outputs-manual
          path: ${{ steps.find_outputs.outputs.run_dir }}
          retention-days: 30
      
      - name: Fail workflow if validation failed
        if: steps.validate.outcome != 'success'
        run: |
          echo "::error::Validation failed for one or more data points"
          echo "Check the validation step output above for detailed error information."
          exit 1

